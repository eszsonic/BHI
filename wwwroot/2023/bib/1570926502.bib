@inproceedings{Kan2310:Dynamic,
AUTHOR    = {Xuan Kan and Antonio Aodong {Chen Gu} and Hejie Cui and Ying Guo and Carl
Yang},
TITLE    = {Dynamic Brain Transformer with Multi-level Attention for Brain Network
Analysis},
BOOKTITLE    = {2023 IEEE EMBS International Conference on Biomedical and Health
Informatics (BHI) (IEEE BHI 2023)},
ADDRESS    = {Pittsburgh, USA},
PAGES    = {4},
DAYS    = {15},
MONTH    = {October},
YEAR    = {2023},
KEYWORDS    = {Dynamic Brain Networks; Deep Learning},
ABSTRACT    = {Recent neuroimaging advancements have highlighted the importance of
network-centric brain analysis, particularly with functional magnetic
resonance imaging. The emergence of Deep Neural Networks has fostered a
substantial interest in predicting clinical outcomes and categorizing
individuals based on brain networks. However, the conventional approach
involving static brain network analysis offers limited potential in
capturing the dynamism of brain function. Although recent studies have
attempted to harness dynamic brain networks, their high dimensionality and
complexity present substantial challenges. This paper proposes a novel
methodology, Dynamic bRAin Transformer (DART), which combines static and
dynamic brain networks for more effective and nuanced brain function
analysis. Our model uses the static brain network as a baseline,
integrating dynamic brain networks to enhance performance against
traditional methods. We innovatively employ attention mechanisms, enhancing
model explainability and exploiting the dynamic brain network's temporal
variations. The proposed approach offers a robust solution to the low
signal-to-noise ratio of blood-oxygen-level-dependent signals, a recurring
issue in direct DNN modeling. It also provides valuable insights into which
brain circuits or dynamic networks contribute more to final predictions. As
such, DRAT shows a promising direction in neuroimaging studies,
contributing to the comprehensive understanding of brain organization and
the role of neural circuits.},
}