<!DOCTYPE html>
<html>
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>MultiTrans: Multi-Branch Transformer Network for Medical Image Segmentation.html</title>
</head>
<body>
<h1> MultiTrans: Multi-Branch Transformer Network for Medical Image Segmentation</h1><br><i><b> Yanhua Zhang; Gabriella Balestra; Zhang Ke; Valentina Giannini; Jingyu Wang; Samanta Rosati</b></i><br><h2>Abstract</h2><br>Convolutional neural networks (CNN) have made promising improvements in automatic medical image segmentation. However, they show limitations in modeling long-range dependency due to the intrinsic locality properties of convolution kernel. Transformer, which is notable for its ability of global context modeling, has been used to remedy the shortcomings of CNN and break its dominance in medical image segmentation. The self-attention module is both memory and computational inefficient, so many methods choose to build their Transformer branch upon largely downsampled feature maps or adopt the tokenized image patches to fit their model into accessible GPUs. This patch-wise operation restricts the network in extracting pixel-level intrinsic structural or dependencies inside each patch, hurting the performance of pixel-level classification tasks. To tackle these issues, we design a memory- and computation-efficient self-attention module to enable reasoning on relatively high-resolution features, promoting the efficiency of learning global information while effective grasping fine spatial details. Furthermore, we design a novel Multi-Branch Transformer architecture to provide hierarchical features for handling objects with variable shapes and sizes in medical images. By building four parallel Transformer branches on different levels of CNN, our hybrid network aggregates both multi-scale global contexts and multi-scale local features. Extensive experiments on two medical image datasets with different modalities demonstrate the superiority and generality of our proposed network. We also conducted exhaustive experiments to demonstrate the effectiveness of our motivation and the architecture design.<br><br><b>Keywords: Abdominal Multi-Organ Segmentation; Cardiac Segmentation; Deep Learning; Efficient Self-Attention; Parallel Transformer Branches<br> <br><h2>Links</h2>[<a href="pdfs/1570918667.pdf">Full text PDF</a>][<a href="bib/1570918667.bib">Bibtex</a>] <br> </html>